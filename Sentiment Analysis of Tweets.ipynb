{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DEEPANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk                                \n",
    "from nltk.corpus import twitter_samples   \n",
    "import matplotlib.pyplot as plt           \n",
    "import random   \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "stopwords=nltk.download('stopwords')\n",
    "import re                                  \n",
    "import string                              \n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\DEEPANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive and negative tweets \n",
    "pos_tweet = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweet = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie([len(pos_tweet), len(neg_tweet)] , labels=['Positives', 'Negative'], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Positive and Negative Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + pos_tweet[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + neg_tweet[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet_set):\n",
    "    for index,i in enumerate(tweet_set):\n",
    "            tweet2=re.sub(r'^RT[\\s]+', '', i) #removal of retweets\n",
    "            # remove hyperlinks\n",
    "            tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "            # only removing the hash # sign from the word\n",
    "            tweet_set[index]=re.sub(r'#', '', tweet2)\n",
    "            \n",
    "    return (tweet_set)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweet=clean_tweet(neg_tweet)\n",
    "pos_tweet=clean_tweet(pos_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "def tocken(tweet_set):\n",
    "    for index, item in enumerate(tweet_set):\n",
    "        tweet_tokens = tokenizer.tokenize(item)\n",
    "        tweet_set[index]=tweet_tokens  \n",
    "    return(tweet_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweet=tocken(pos_tweet)\n",
    "neg_tweet=tocken(neg_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "def stop(tweet_set):\n",
    "    for index,i in enumerate(tweet_set):\n",
    "        clean=[]\n",
    "        for word in i:       \n",
    "            if (word not in stopwords_english and word not in string.punctuation): \n",
    "                clean.append(word)\n",
    "                tweet_set[index]=clean\n",
    "            \n",
    "    return (tweet_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweet=stop(pos_tweet)\n",
    "neg_tweet=stop(neg_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "\n",
    "def stemm(tweet_set):\n",
    "    for index, i in enumerate(tweet_set):\n",
    "        tweets_stem=[]\n",
    "        for word in i:\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_stem.append(stem_word)\n",
    "        tweet_set[index]=tweets_stem\n",
    "        \n",
    "    return (tweet_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweet=stemm(pos_tweet)\n",
    "neg_tweet=stemm(neg_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in pos_tweet:\n",
    "        corpus.append(' '.join(i))\n",
    "pos=pd.DataFrame(corpus,columns=['Tweets'])\n",
    "pos['output']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in neg_tweet:\n",
    "        corpus.append(' '.join(i))\n",
    "neg=pd.DataFrame(corpus,columns=['Tweets'])\n",
    "neg['output']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_df=pd.concat([pos,neg],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alotting the 1 and 0 to positive and negative words respectively\n",
    "\n",
    "labels = np.append(np.ones((len(pos_tweet))), np.zeros((len(neg_tweet))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Frequency Table of Words from Positive and Negative Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pos_tweet+neg_tweet\n",
    "def frqs(tweet,labels):\n",
    "    yslist = np.squeeze(labels).tolist()\n",
    "    print(yslist)\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in tweet:\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frqs=frqs(tweets,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_set=[]\n",
    "for i in tweets:\n",
    "    for j in i:\n",
    "        new_set.append(j) \n",
    "new_set=list(set(new_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub(word):\n",
    "            dat=[]\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "\n",
    "            if (word, 1) in word_frqs:\n",
    "                pos = word_frqs[(word, 1)]\n",
    "\n",
    "            if (word, 0) in word_frqs:\n",
    "                neg = word_frqs[(word, 0)]\n",
    "\n",
    "            return(1, pos, neg)\n",
    "\n",
    "\n",
    "def vect(new_set1):\n",
    "    datalist = []\n",
    "    for i in new_set1:\n",
    "        new=[]\n",
    "        try:\n",
    "            datalist.append(sub(i))\n",
    "        except:\n",
    "            for j in i:\n",
    "                new.append(sub(j))\n",
    "            datalist.append(new)\n",
    "            \n",
    "    return (datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=vect(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data, columns=['Bais','Positive','Negative'])\n",
    "df['Sentiment']=0\n",
    "for i in df.index:\n",
    "    if (df['Positive'][i]>=df['Negative'][i]):\n",
    "        df['Sentiment'][i]=1\n",
    "    if df['Positive'][i]<df['Negative'][i]:\n",
    "        df['Sentiment'][i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Bais', 'Positive', 'Negative']].values # Get only the numerical values of the dataframe\n",
    "Y = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(key):       \n",
    "    data=[]\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in key:\n",
    "        if (word, 1) in word_frqs:\n",
    "            pos = word_frqs[(word, 1)]\n",
    "\n",
    "        if (word, 0) in word_frqs:\n",
    "            neg = word_frqs[(word, 0)]\n",
    "\n",
    "        data.append([word, pos, neg])\n",
    "        \n",
    "        \n",
    "    return (data)\n",
    "\n",
    "def plott(data):\n",
    "\n",
    "    #Visualisation of words on Log Scale\n",
    "    fig, ax = plt.subplots(figsize = (8, 8))\n",
    "    x = np.log([x[1] + 1 for x in data])  \n",
    "    y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "    # Plot a dot for each pair of words\n",
    "    ax.scatter(x, y)  \n",
    "\n",
    "    # assign axis labels\n",
    "    plt.xlabel(\"Log Positive count\")\n",
    "    plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "    # Add the word as the label at the same position as you added the points just before\n",
    "    for i in range(0, len(data)):\n",
    "        ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "    ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "plott(visualise(k_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing The Model And Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=lr.predict(xtest)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 1500)\n",
    " \n",
    "X = cv.fit_transform(naive_df['Tweets']).toarray()\n",
    "y = naive_df.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "           X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "classifier = GaussianNB();\n",
    "classifier.fit(X_train, y_train)\n",
    " \n",
    "# predicting test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    " \n",
    "# making the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing By Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comment(comment):\n",
    "    \n",
    "    clean=clean_tweet(comment)\n",
    "    tockenise=tocken(clean)\n",
    "    stop_clean=stop(tockenise)\n",
    "    stemm_clean=stemm(stop_clean)\n",
    "    \n",
    "        \n",
    "    freq_test=vect(stemm_clean)\n",
    "    \n",
    "    for i in freq_test:\n",
    "        test=pd.DataFrame(i, columns=['Bais','Positive','Negative'])\n",
    "        pred=lr.predict(test)\n",
    "    if max(list(pred),key = list(pred).count)==1:\n",
    "        print('\\033[92m' + \"Positive Comment\")\n",
    "    else:\n",
    "        print('\\033[91m'+\"Negative Comment\")\n",
    "        \n",
    "    for i in stemm_clean:\n",
    "        plott(visualise(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[input(\"Enter The Comment You Want To Check: \")]\n",
    "test_comment(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
